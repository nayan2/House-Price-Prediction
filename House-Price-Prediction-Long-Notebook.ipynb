{"nbformat":4,"nbformat_minor":5,"metadata":{"accelerator":"TPU","colab":{"name":"House-Proce-Prediction.ipynb","provenance":[],"collapsed_sections":["p_2xoyOifHN0","IjcirM0ufHN0","83d2d069"]},"interpreter":{"hash":"6d46af94c2bbce495f1e668725902fa517c90b1782bcfe2fce0dd9868df553d3"},"kernelspec":{"display_name":"Python 3.8.8 64-bit ('base': conda)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"papermill":{"default_parameters":{},"duration":28.773459,"end_time":"2021-09-20T18:53:06.971789","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2021-09-20T18:52:38.198330","version":"2.3.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"107d2f23"},"source":["# Moscow Housing Dataset\n","\n","In this challenge you will predict the prices of apartments in Moscow. \n","\n","Real estate is a popular investment vehicle and can be pretty lucurative. Whether you're an individual looking for a place to call home or an investor looking for a profitable opportunity, being able to appraise assets accurately - or at least better than the next guy - can be of huge value.\n","\n","We have procured a dataset consisting of information about over 33,000 apartments in Moscow. It contains around 30 variables that encode data such as location, facilities, and building information. Your task is to do analysis, feature engineering, and ultimately create models that can reliably predict the listed price of both low- and high-end apartments."],"id":"107d2f23"},{"cell_type":"code","metadata":{"id":"o0-w9EYaZsFD"},"source":["%%HTML\n","<style type=\"text/css\">\n","table.dataframe td, table.dataframe th {\n","    border: 1px  black solid !important;\n","}\n","</style>"],"id":"o0-w9EYaZsFD","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KLs3EqVRfb-6"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pip install optuna\n","!pip install pycaret\n","!pip install catboost\n","!pip install category_encoders"],"id":"KLs3EqVRfb-6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"df1c9046"},"source":["import json\n","import pandas as pd \n","import numpy as np \n","import matplotlib.pyplot as plt \n","import seaborn as sns\n","import optuna\n","\n","np.random.seed(123)\n","sns.set_style('darkgrid')\n","pd.set_option('display.max_colwidth', None)\n","pd.set_option(\"display.max_columns\", None)\n","base_path = '/content/drive/MyDrive/Colab Notebooks/House-Price-Data'"],"id":"df1c9046","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1c572e1d"},"source":["## 1: Data\n","\n","We begin this tutorial by showing how you can download the material and giving a description of the data you will be working with.\n","\n","### Download (with Kaggle API)\n","\n","The data can be download through the Kaggle API. If you've obtained the data in some other fashion (e.g. by downloading it through the web page) then you can skip this step. Just ensure that all the data is located in a sub-directory of the directory this notebook is located in called `data`. \n","\n","To download the data with the Kaggle API, you'll need the _API client_ and an _API key_. Full documentation on how to use it is available from [the official github repository](https://github.com/Kaggle/kaggle-api), but an abriged version is given below.\n","\n","The client can be installed through pip/anaconda.\n","\n","```bash\n","pip install kaggle\n","conda install -c conda-forge kaggle\n","```\n","\n","The API key can be generated from the kaggle website. Go to `Your Profile` (in sidebar accessible from icon in the top right corner), navigate to the `Account` tab, scroll down to the `API` section, and click the `Create New API Token` button. This should generate an API token and start a download of a json file called `kaggle.json`. \n","\n","Once the download is complete, you must move the file to a special kaggle folder.\n","\n","\n","```bash\n","mkdir -p ~/.kaggle   # Creates a '.kaggle' directory in your homefolder if it doesn't exist\n","mv ~/Downloads/kaggle.json ~/.kaggle   # Moves 'kaggle.json' (API key) to ~/.kaggle folder\n","chmod 600 ~/.kaggle/kaggle.json  # Sets read/write access to API key file to owner only\n","```\n","\n","**Note**: on Windows, the default location for the kaggle home folder might look something like `C:\\Users\\<Windows-username>\\.kaggle\\kaggle.json`.\n","\n","Once the API is set up you can download the competition data."],"id":"1c572e1d"},{"cell_type":"code","metadata":{"id":"24ea728c"},"source":["# -- Uncomment and run the shell commands below to download and unzip\n","\n","# # Create a data folder (if it does not exist)\n","# !mkdir -p ./data \n","# # Download the competition data as a zip\n","# !kaggle competitions download -c moscow-housing-tdt4173 -p ./data/ \n","# # Unzip the competition data (alternatively use another unzipping software you have installed)\n","# !unzip -o -d data/ data/moscow-housing-tdt4173.zip\n","\n","# -- Here we just create a symlink because this code is being run on a kaggle \n","#    machine where the data is already downloaded to /kaggle/input/...\n","\n","# Note: Don't run this command if you run this notebook locally\n","# !ln -s /kaggle/input/moscow-housing-tdt4173 ./data"],"id":"24ea728c","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"77feda61"},"source":["### Directory structure\n","\n","The dataset comes as 6 files plus a street map of moscow. We will have a closer look at the data later, but a brief rundown of the contents is given below.\n","\n","- `apartments_(train|test).csv` contains information about specific apartments. Each row in this table corresponds to one datapoint that you will make a prediction for. The train file also contains a column for the listed apartment price, i.e. the ground truth to the variable you will be predicting.\n","- `buildings_(train|test).csv` contains suplementary information about the building each apartment is located in. \n","- `(apartments|buildings)_meta.json` contains metadata about the columns found in the apartment and building tables, including a brief description, the datatype, and categories (where applicable)."],"id":"77feda61"},{"cell_type":"code","metadata":{"id":"84486069"},"source":["# !ls ./data | sort"],"id":"84486069","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"10031e2f"},"source":["def describe_column(meta):\n","    \"\"\"\n","    Utility function for describing a dataset column (see below for usage)\n","    \"\"\"\n","    def f(x):\n","        d = pd.Series(name=x.name, dtype=object)\n","        m = next(m for m in meta if m['name'] == x.name)\n","        d['Type'] = m['type']\n","        d['#NaN'] = x.isna().sum()\n","        d['Description'] = m['desc']\n","        if m['type'] == 'categorical':\n","            counts = x.dropna().map(dict(enumerate(m['cats']))).value_counts().sort_index()\n","            d['Statistics'] = ', '.join(f'{c}({n})' for c, n in counts.items())\n","        elif m['type'] == 'real' or m['type'] == 'integer':\n","            stats = x.dropna().agg(['mean', 'std', 'min', 'max'])\n","            d['Statistics'] = ', '.join(f'{s}={v :.1f}' for s, v in stats.items())\n","        elif m['type'] == 'boolean':\n","            counts = x.dropna().astype(bool).value_counts().sort_index()\n","            d['Statistics'] = ', '.join(f'{c}({n})' for c, n in counts.items())\n","        else:\n","            d['Statistics'] = f'#unique={x.nunique()}'\n","        return d\n","    return f\n","\n","def describe_data(data, meta):\n","    desc = data.apply(describe_column(meta)).T\n","    desc = desc.style.set_properties(**{'text-align': 'left'})\n","    desc = desc.set_table_styles([ dict(selector='th', props=[('text-align', 'left')])])\n","    return desc "],"id":"10031e2f","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"517b049a"},"source":["### Appartments\n","\n","We first load the training split of the apartment data. This file contains information that is specific to each apartment. As previously stated, each row in this table correspond to a unique datapoint and the price column is the variable you will be trying to predict in this challenge. We use the `describe_data` function decleared in the cell above to map the apartment data to its metadata and generate summary statistics. Note that several of the features have a substantial amount of missing values (`#NaN`). We will get back to this later in this notebook."],"id":"517b049a"},{"cell_type":"code","metadata":{"id":"a5189fed"},"source":["apartments = pd.read_csv(f'{base_path}/apartments_train.csv')\n","print(f'Loaded {len(apartments)} apartments')\n","with open(f'{base_path}/apartments_meta.json') as f: \n","    apartments_meta = json.load(f)\n","describe_data(apartments, apartments_meta)"],"id":"a5189fed","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1f7e065f"},"source":["### Buildings\n","\n","Next we load the training split of the building data. In it, you will find additional building-level information to supplement the apartment data. This includes its physical location, its current state and construction details, as well as shared facilities such as elevators and parking. This file is a bit smaller than the apartment file because several apartments can map to the same building. As we will see in the next section, you can use the `id` column to map buildings to their respective apartments."],"id":"1f7e065f"},{"cell_type":"code","metadata":{"id":"5cf54604"},"source":["buildings = pd.read_csv(f'{base_path}/buildings_train.csv')\n","print(f'Loaded {len(buildings)} buildings')\n","with open(f'{base_path}/buildings_meta.json') as f:\n","    buildings_meta = json.load(f)\n","buildings.head()\n","describe_data(buildings, buildings_meta)"],"id":"5cf54604","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"32fGFUaffHNx"},"source":["column_description = pd.concat([pd.DataFrame(buildings_meta), pd.DataFrame(apartments_meta)], axis=0)[['name', 'type', 'cats']]"],"id":"32fGFUaffHNx","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c661c341"},"source":["## Combined\n","\n","Most datascience tools and pipelines for tabular data assumes a single table (sometimes referred to as a design matrix) to work with. To supplement the apartment datapoints with building information, we map building rows to apartment rows based on the `apartment.building_id == building.id` relationship with the `pd.merge` function. We use a `left` join to preserve all the apartment rows, while possibly duplicating building rows that correspond to more than one apartment."],"id":"c661c341"},{"cell_type":"code","metadata":{"id":"7c25edda"},"source":["print(f'All apartments have an associated building: {apartments.building_id.isin(buildings.id).all()}')\n","data = pd.merge(apartments, buildings.set_index('id'), how='left', left_on='building_id', right_index=True)\n","data.head()"],"id":"7c25edda","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3aefc7bb"},"source":["## 2: Objective\n","\n","Your objective is to predict the value of the `price` column based on all the other columns. You are free to approach this regression problem with all the data science tricks you know or learn about during the project, just make sure that your process is well-documented and explained in your deliverables."],"id":"3aefc7bb"},{"cell_type":"markdown","metadata":{"id":"ca1958d9"},"source":["### Evaluation Metric\n","\n","It is common to evaluate regression problems according to some deviation measure of the error (difference) between the predictions and the ground truth values. Typical choices are Mean Squared Error (MSE) and its square root, the Root Mean Squared Error (RMSE). \n","\n","However, both of these measures are quite sensitive to extreme values and work best if the typical scale of prediction errors are consistent across the dataset. This is not likely to be the case here because the price variable ranges from around 1 million rubles to over 2 billion rubles. This means that a, say 10%, prediction error would matter a lot more if it is for one of the expensive apartments than if it is for one of the cheaper ones. Consequently, we will use a variation that takes a log transform of the target variable before computing prediction errors.\n","\n","**TL;DR**: submissions for this problem will be evaluated according to the `Root Mean Squared Log Error` (RMSLE):\n","\n","- $\\text{RMSLE}(y, \\hat{y}) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}{(\\log(1 + \\hat{y_i}) - \\log(1 + y_i))^2}}$\n","\n","In the equation above, ${y_i}$ corresponds to the ground truth for datapoint $i$, $\\hat{y_i}$ corresponds to the predicted value for datapoint $i$, and $n$ denotes the total number of datapoints (size of $y$, $\\hat{y}$). See the cell below for an implementation."],"id":"ca1958d9"},{"cell_type":"code","metadata":{"id":"43eccab6"},"source":["def root_mean_squared_log_error(y_true, y_pred):\n","    # Alternatively: sklearn.metrics.mean_squared_log_error(y_true, y_pred) ** 0.5\n","    assert (y_true >= 0).all() \n","    assert (y_pred >= 0).all()\n","    log_error = np.log1p(y_pred) - np.log1p(y_true)  # Note: log1p(x) = log(1 + x)\n","    return np.mean(log_error ** 2) ** 0.5"],"id":"43eccab6","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e1b36b7c"},"source":["### Train-Test Split\n","\n","We have already made a train/test split that will be used to evaluate your submission. Concretely, you are given:\n","- ~23k training samples **with** price data that you can use to select and fit models\n","- ~10k testing samples **without** price data that we will use to evaluate your models\n","\n","The split have been desiged so that you can expect roughly the same distribution of data during evaluation. Specifically, we have made sure to stratify it with respect to the price range and location of apartments. In the cell below, we load the test split in an equivalent manner to how the training data was loaded."],"id":"e1b36b7c"},{"cell_type":"code","metadata":{"id":"329148da"},"source":["apartments_test = pd.read_csv(f'{base_path}/apartments_test.csv')\n","buildings_test = pd.read_csv(f'{base_path}/buildings_test.csv')\n","print(f'All test apartments have an associated building: {apartments_test.building_id.isin(buildings_test.id).all()}')\n","data_test = pd.merge(apartments_test, buildings_test.set_index('id'), how='left', left_on='building_id', right_index=True)\n","print(f'Number of train samples: {len(data)}')\n","print(f'Number of test samples:  {len(data_test)}')"],"id":"329148da","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p_2xoyOifHN0"},"source":["## Steps\n","- Cleaning\n","- Feature engineering\n","- Feature Transformation\n","- Encoding\n","- Scaling\n","- Target Transformation\n","\n","- Model selection\n","- Hyperparameter optimization\n","- Ensembling\n","- Feature selection"],"id":"p_2xoyOifHN0"},{"cell_type":"markdown","metadata":{"id":"IjcirM0ufHN0"},"source":["## To solve\n","- data is not uniform\n","- data is unstrcture\n","- data contains outliers\n","- data has some error, anomaly, outliers \n"],"id":"IjcirM0ufHN0"},{"cell_type":"markdown","metadata":{"id":"9fdZlVgqfHN0"},"source":["## Fix outliers data with min and median\n","* Initally we tried to remove the outliers but, removing the data effected the mode performace\n","\n","\n","\n","--------------------------------------------------------\n","\n","\n","* Rooms and bathroom combination anomaly - Means An aparament cant have like 1 room and 3 bathroom\n","* Price per square feet anomaly\n","* Living area and kitchen area anomaly"],"id":"9fdZlVgqfHN0"},{"cell_type":"code","metadata":{"id":"ZTwtDIfxWc-H"},"source":["len(data[((data['bathrooms_shared'] + data['bathrooms_private']) / data['rooms']) >= 3.0][['rooms', 'bathrooms_private', 'bathrooms_shared']])"],"id":"ZTwtDIfxWc-H","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A4LOgi7dJkbH"},"source":["area_description = data[((data['area_kitchen'] + data['area_living']) < data['area_total']) & (data['area_living'] < data['area_total']) & (data['area_kitchen'] < data['area_total']) & (data['area_kitchen'] > 5.0) & (data['area_living'] > 20.0)].groupby(['district'])[['area_living', 'area_kitchen', 'area_total']].mean()\n","(general_area_living, general_area_kitchen, general_area_total) = data[((data['area_kitchen'] + data['area_living']) < data['area_total']) & (data['area_living'] < data['area_total']) & (data['area_kitchen'] < data['area_total']) & (data['area_kitchen'] > 5.0) & (data['area_living'] > 20.0)][['area_living', 'area_kitchen', 'area_total']].mean()\n","area_description = area_description.reset_index(level=0, drop=True)\n","area_description['living_percentage'] = (area_description['area_living'] / area_description['area_total']) * 100\n","area_description['kitchen_percentage'] = (area_description['area_kitchen'] / area_description['area_total']) * 100\n","area_description['general_living_percentage'] = (general_area_living / general_area_total)  * 100\n","area_description['general_kitchen_percentage'] = (general_area_kitchen / general_area_total) * 100"],"id":"A4LOgi7dJkbH","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PVLd2iHbJmvT"},"source":["def trnsform_outliers(row, area_description):\n","    kitchen_percentage = area_description.loc[int(row['district']), 'kitchen_percentage'] if not pd.isna(row['district']) else area_description.loc[0, 'general_kitchen_percentage']\n","    living_percentage = area_description.loc[int(row['district']), 'living_percentage'] if not pd.isna(row['district']) else area_description.loc[0, 'general_living_percentage']\n","    \n","    if row['area_kitchen'] > row['area_total']:\n","        row['area_kitchen'] = row['area_total'] * kitchen_percentage\n","\n","    if row['area_living'] > row['area_total']:\n","        row['area_living'] = row['area_total'] * living_percentage\n","\n","    if ((row['area_living'] + row['area_kitchen']) > row['area_total']):\n","        row['area_kitchen'] = row['area_total'] * kitchen_percentage\n","        row['area_living'] = row['area_total'] * living_percentage\n","\n","    return row"],"id":"PVLd2iHbJmvT","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J6WdyKNNJoSG"},"source":["data = data.apply(lambda row: trnsform_outliers(row, area_description), axis=1)"],"id":"J6WdyKNNJoSG","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zVHI_MQmfHN0"},"source":["## Merge the data"],"id":"zVHI_MQmfHN0"},{"cell_type":"code","metadata":{"id":"qckc8xmHfHN1"},"source":["data_combined = pd.concat([data, data_test]).reset_index(drop=True)\n","data_combined1 = data_combined.copy()"],"id":"qckc8xmHfHN1","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jaf51mUhfHN1"},"source":["# Clean up the Ids, and string columns\n","target = data['price']\n","data_combined2 = data_combined1.drop(['id', 'building_id', 'price', 'address', 'street'], axis= 1)"],"id":"Jaf51mUhfHN1","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wbM4RqD3UgyJ"},"source":["data_combined2 = data_combined2.drop([\n","    'layout', \n","    'windows_court', \n","    'windows_street', \n","    'phones',\n","    'elevator_passenger', \n","    'garbage_chute', \n","    'heating',\n","    'rooms'\n","], axis=1)"],"id":"wbM4RqD3UgyJ","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7WGtQAjWfHN1"},"source":["# Imputer categorical column data\n","for column in [\n","    'material',\n","    'parking',\n","    'seller',\n","    'condition',\n","    'heating',\n","    'layout',\n","    'windows_court',\n","    'windows_street',\n","    'garbage_chute',\n","    'elevator_passenger',\n","    'new', \n","    'district',     \n","    'elevator_without',\n","    'elevator_service'\n","]:\n","    data_combined2[column] = data_combined2[column].fillna(data_combined2[column].mode()[0])"],"id":"7WGtQAjWfHN1","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hq93g8wTfHN2"},"source":["# Imputer numerical and boolean data using KNN\n","from pandas import DataFrame\n","from sklearn.neighbors import KNeighborsRegressor\n","import math\n","\n","def ImputerNumericData(df: DataFrame, non_na_columns, target: str) -> DataFrame:\n","    df = df.copy()\n","\n","    X_train = df.loc[df[target].isna() == False, non_na_columns]\n","    y_train = df.loc[df[target].isna() == False, target]\n","    X_test = df.loc[df[target].isna() == True, non_na_columns]\n","\n","    knn = KNeighborsRegressor()\n","    knn.fit(X_train, y_train)\n","\n","    y_pred = pd.DataFrame(knn.predict(X_test), columns=['Res'])\n","    if len(column_description[(column_description['name'] == target) & (column_description['type'] == 'integer')]) >= 1:\n","        y_pred['Res'] = y_pred.apply(lambda row: math.ceil(row['Res']) if row['Res'] - int(row['Res']) > .5 else math.floor(row['Res']), axis=1)\n","\n","    df.loc[df[target].isna() == True, target] = y_pred['Res'].values\n","\n","    return df"],"id":"hq93g8wTfHN2","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"74zafU0XfHN2"},"source":["non_na_columns = data_combined2.loc[:, data_combined2.dtypes != 'object'].loc[:, data_combined2.isna().sum() == 0].columns\n","\n","for column in [\n","    'latitude',\n","    'longitude',\n","    'area_kitchen',\n","    'area_living',\n","    'ceiling',\n","    'bathrooms_shared',\n","    'bathrooms_private',\n","    'balconies',\n","    'loggias',\n","    'constructed',\n","    'phones'\n","]:\n","    data_combined2 = ImputerNumericData(data_combined2, non_na_columns, column)"],"id":"74zafU0XfHN2","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"78a2c284"},"source":["## 3: Basic Exploratory Data Analysis (EDA)\n","\n","In the sections below, we provide brief analyses of some of the more pertient aspects of the data. Doing good EDA is an artform in itself and we expect you to go beyond what is provided here."],"id":"78a2c284"},{"cell_type":"markdown","metadata":{"id":"85397d8f"},"source":["### Missing Data\n","\n","We begin by taking a closer look at the missing values. As can be seen in the plots below, about half of the features have almost no missing values, but the remainig half is a mixed bag ranging from around 20% missing values to more than 70%. For [various reasons](https://en.wikipedia.org/wiki/Missing_data#Types), this is a common problem setting in machine learning that can be tackled in several ways. Perhaps the easiest way to go about it is to simply ignore all features with missing values. However, that naturally comes with the risk of missing out on useful information that could benefit the model. You can also drop rows with missing values from the training set, but remember that you still have to make predictions for all the rows in the test set.\n","\n","The process of filling in missing data entries is referred to as _imputation_. A simple way to do this for real-valued data is to replace missing values by the sample mean (or median) of the data that is present. For categorical features, you can create an extra category for \"missing\", and boolean features can be converted to a categorical True/False/DontKnow. However, there are also more sophisticated approaches out there that you might want to look into. For more information on the topic, [Scikit Learn's documentation](https://scikit-learn.org/stable/modules/impute.html) is a good place to start."],"id":"85397d8f"},{"cell_type":"code","metadata":{"id":"2a0999bc"},"source":["fig, (ax1, ax2) = plt.subplots(figsize=(16, 4), ncols=2, dpi=100)\n","print(f'Number of missing price entries in train data: {data.price.isna().sum()}')\n","print(f'Training set features with any missing value: {data.isna().any().sum()}/{data.shape[1]}')\n","print(f'Testing set features with any missing value: {data_test.isna().any().sum()}/{data_test.shape[1]}')\n","data_combined2.isna().mean().plot.bar(ax=ax1, title='Fraction of NaN values in the training set')\n","data_test.isna().mean().plot.bar(ax=ax2, title='Fraction of NaN values in the testing set')"],"id":"2a0999bc","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OZdnkEawfHN3"},"source":["## Feature Engineering"],"id":"OZdnkEawfHN3"},{"cell_type":"code","metadata":{"id":"z8zIZWHQfHN3"},"source":["from geopy.distance import geodesic\n","\n","data_combined2[\"bathrooms_total\"] = data_combined2[\"bathrooms_shared\"] + data_combined2[\"bathrooms_private\"]\n","data_combined2[\"kitchen_percentage\"] = data_combined2[\"area_kitchen\"] / data_combined2[\"area_total\"] * 100\n","data_combined2[\"living_area_percentage\"] = data_combined2[\"area_living\"] / data_combined2[\"area_total\"] * 100\n","data_combined2[\"distance_from_city_center\"] = data_combined2.apply(lambda row: geodesic((55.75106861094267, 37.61617397889437), (row['latitude'], row['longitude'])).miles, axis=1) "],"id":"z8zIZWHQfHN3","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZcdrIiBeYO1Y"},"source":["## Calculate the nearst restaurant,school,airport,shopping-mall,bus-station etc\n","- Using google place/nearby API"],"id":"ZcdrIiBeYO1Y"},{"cell_type":"code","metadata":{"id":"MiBH1BTyYoOq"},"source":["# nearby_places_list = ['airport']\n","# for place in nearby_places_list:\n","#     content = ''\n","#     with open(f'data/{place.replace(\"+\", \"-\")}.txt', \"r\" ) as record_file:\n","#         for line in record_file:\n","#             try:\n","#                 if line.startswith('FAILED'):\n","#                     row = data_combined2.iloc[int(line.split(\"-\")[1]), :]\n","#                     url = f\"https://maps.googleapis.com/maps/api/place/nearbysearch/json?location={row['latitude']}%2C{row['longitude']}&rankby=distance&type={place}&key=AIzaSyAJiM-s2-iVVhJ8I7u7ZSZPi2zLbhqwPCM\"\n","#                     response = requests.request(\"GET\", url, headers={}, data={})\n","#                     result = sorted([geodesic((row['latitude'], row['longitude']), (result['geometry']['location']['lat'], result['geometry']['location']['lng'])).miles for result in response.json()['results']])\n","#                     content += f\"{result[0]}\\n\"\n","#                     print('Recovered')\n","#                 else:\n","#                     content += line\n","#             except Exception as ex:\n","#                     print(ex)\n","#                     content += line\n","#     with open(f'data/{place.replace(\"+\", \"-\")}.txt', \"w\" ) as record_file:\n","#         record_file.write(content)"],"id":"MiBH1BTyYoOq","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z1yvnW34Yo5I"},"source":["# nearby_places_list = ['restaurant', 'school', 'airport', 'shopping+mall', 'groceries']\n","# for place in nearby_places_list:\n","#     print(f'-------Processing the feature --------> {place}')\n","\n","#     with open(f'data/{place.replace(\"+\", \"-\")}.txt', \"a+\") as record_file:\n","#         num_lines = sum(1 for line in record_file)\n","#         data_combined_split = data_combined2.iloc[num_lines:, :].reset_index(drop=True)\n","#         with tqdm(total=len(data_combined_split)) as pbar:\n","#             for index, (_, row) in enumerate(data_combined_split.iterrows()):\n","#                 try:\n","#                     url = f\"https://maps.googleapis.com/maps/api/place/nearbysearch/json?location={row['latitude']}%2C{row['longitude']}&radius=50000&type={place}&key=AIzaSyAJiM-s2-iVVhJ8I7u7ZSZPi2zLbhqwPCM\"\n","#                     response = requests.request(\"GET\", url, headers={}, data={})\n","#                     result = sorted([geodesic((row['latitude'], row['longitude']), (result['geometry']['location']['lat'], result['geometry']['location']['lng'])).miles for result in response.json()['results']])\n","#                     record_file.write(f\"{result[0]}\\n\")\n","#                 except Exception as ex:\n","#                     print(ex)\n","#                     record_file.write(f\"FAILED-{index}\\n\")\n","#                 finally:\n","#                     pbar.update(1)\n","\n","# data_combined2"],"id":"z1yvnW34Yo5I","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S892N7H2YP7U"},"source":["nearby_places_list = ['restaurant', 'school', 'shopping+mall', 'groceries']\n","for place in nearby_places_list:\n","    with open(f'{base_path}/{place.replace(\"+\", \"-\")}.txt', \"r\" ) as record_file:\n","        records = record_file.read().split('\\n')[:-1]\n","        data_combined2[f'distance_from_{place.replace(\"+\", \"-\")}'] = records\n","        data_combined2[f'distance_from_{place.replace(\"+\", \"-\")}'] = data_combined2[f'distance_from_{place.replace(\"+\", \"-\")}'].astype('float')"],"id":"S892N7H2YP7U","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1L6EjehPYbOY"},"source":["## Marking the floor\n","- Is top floor, with lift\n","- If between 2 - top-1, with lift\n","- If more than 5 storey building without lift\n","- If ground floor"],"id":"1L6EjehPYbOY"},{"cell_type":"code","metadata":{"id":"lZxcigwEYc7c"},"source":["def Get_floor_marking(row: any):\n","    score = 100\n","    if row['elevator_without'] == 1.0 and row['stories'] > 5:\n","        score -= 20\n","    elif row['elevator_without'] == 1.0:\n","        score -= 10\n","    \n","    if row['stories'] == row['floor'] or row['floor'] <= 0:\n","        score -= 10\n","    return score"],"id":"lZxcigwEYc7c","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FcsbBtO6YgRw"},"source":["data_combined2['floor_score'] = data_combined2.apply(lambda row: Get_floor_marking(row), axis=1)\n","data_combined2 = data_combined2.drop('floor', axis=1)"],"id":"FcsbBtO6YgRw","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_D1qPGzQv01n"},"source":["## Remove the Geo as doing normalization on GEO data is not a good choice"],"id":"_D1qPGzQv01n"},{"cell_type":"code","metadata":{"id":"Vm3JsCdKvpTE"},"source":["data_combined2 = data_combined2.drop(['latitude', 'longitude'], axis=1)"],"id":"Vm3JsCdKvpTE","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9mpNDJtSfHN3"},"source":["## Feature Transformation"],"id":"9mpNDJtSfHN3"},{"cell_type":"code","metadata":{"id":"Tyn6AJLGfHN3"},"source":["import scipy.stats\n","\n","for column in [\n","    \"constructed\",\n","    \"area_total\",\n","    \"area_kitchen\",\n","    \"area_living\",\n","    \"floor\",\n","    \"ceiling\",\n","    \"bathrooms_shared\",\n","    \"bathrooms_private\",\n","    \"bathrooms_total\",\n","    \"kitchen_percentage\",\n","    \"living_area_percentage\",\n","    \"balconies\",\n","    \"loggias\",\n","    \"phones\"\n","    \"rooms\",\n","    \"distance_from_city_center\",\n","]:\n","    if abs(scipy.stats.skew(data_combined2[column])) > 0.5:\n","        data_combined2[column] = np.log1p(data_combined2[column])"],"id":"Tyn6AJLGfHN3","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ui74cJnTfHN4"},"source":["## Remove low data points"],"id":"Ui74cJnTfHN4"},{"cell_type":"code","metadata":{"id":"7AMVKXEnfHN4"},"source":["# Remove unnecessary data/columns before encode\n","# street_count_less_than_two = data_combined2.groupby('street')['street'].agg('count').sort_values(ascending=False)\n","# street_count_less_than_two = street_count_less_than_two[street_count_less_than_two <= 10]\n","# data_combined2['street'] = data_combined2['street'].apply(lambda x: 'other' if x in street_count_less_than_two else x)"],"id":"7AMVKXEnfHN4","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fchX0xwkMEG-"},"source":["### Regression Target\n","\n","Next we examine the target variable; the listed apartment price. As previously mentioned, there's a big spread in prices. Plotting a histogram of the raw value yields a sharp peak (relatively) close to zero with a long tail stretching past 2.5 billion rubles. A log transform makes it a bit easier to tell what is going on. We use a `log10` transform so that the number on the x-axis correspond to the number of digits in the price. A distinct mode can be observed around 7.0 (10 million rubles) with the distribution tapering off quickly to the left and more slowly to the right."],"id":"fchX0xwkMEG-"},{"cell_type":"code","metadata":{"id":"k_xYnixvMFYT"},"source":["fig, (ax1, ax2) = plt.subplots(figsize=(16, 4), ncols=2, dpi=100)\n","sns.histplot(target.rename('price / rubles'), ax=ax1)\n","ax1.set_title('Distribution of raw train set prices')\n","sns.histplot(np.log10(target).rename('log10(price)'), ax=ax2)\n","ax2.set_title('Distribution of train set prices after log transform')\n","\n","log_target = np.log10(target)"],"id":"k_xYnixvMFYT","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GemrZgAVMNRc"},"source":["### Geographical Considerations\n","\n","Here we will have a rudimentary look at correlations between features and the target variable. Specifically, we will investigate the relationship between an apartment's location and its price. Anyone who's ever been out apartment shopping knows that proximity to popular facilities often plays a huge role in what prices you should expect. We visualize the relationship by creating a scatterplot between the latitude and longitude coordinate features and colorcode the dots based on the apartment price. To further contextualize the data, we also add a backdrop of Moscow exported from [Open Street Map](https://www.openstreetmap.org/#map=10/55.7515/37.4998)."],"id":"GemrZgAVMNRc"},{"cell_type":"code","metadata":{"id":"sS20OhWmMMn2"},"source":["def plot_map(data, ax=None, s=5, a=0.75, q_lo=0.0, q_hi=0.9, cmap='autumn', column='price', title='Moscow apartment price by location'):\n","    data = data[['latitude', 'longitude', column]].sort_values(by=column, ascending=True)\n","    backdrop = plt.imread(f'{base_path}/moscow.png')\n","    backdrop = np.einsum('hwc, c -> hw', backdrop, [0, 1, 0, 0]) ** 2\n","    if ax is None:\n","        plt.figure(figsize=(12, 8), dpi=100)\n","        ax = plt.gca()\n","    discrete = data[column].nunique() <= 20\n","    if not discrete:\n","        lo, hi = data[column].quantile([q_lo, q_hi])\n","        hue_norm = plt.Normalize(lo, hi)\n","        sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(lo, hi))\n","        sm.set_array([])\n","    else:\n","        hue_norm = None \n","    ax.imshow(backdrop, alpha=0.5, extent=[37, 38, 55.5, 56], aspect='auto', cmap='bone', norm=plt.Normalize(0.0, 2))\n","    sns.scatterplot(x='longitude', y='latitude', hue=data[column].tolist(), ax=ax, s=s, alpha=a, palette=cmap,linewidth=0, hue_norm=hue_norm, data=data)\n","    ax.set_xlim(37, 38)    # min/max longitude of image \n","    ax.set_ylim(55.5, 56)  # min/max latitude of image\n","    if not discrete:\n","        ax.legend().remove()\n","        ax.figure.colorbar(sm)\n","    ax.set_title(title)\n","    return ax, hue_norm\n","\n","plot_map(data)"],"id":"sS20OhWmMMn2","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mYUwYTgEMWY3"},"source":["## splitting the data"],"id":"mYUwYTgEMWY3"},{"cell_type":"code","metadata":{"id":"fBnaglwvMWIl"},"source":["train_final = data_combined2.loc[:data.index.max(), :]\n","test_final = data_combined2.loc[data.index.max() + 1:, :].reset_index(drop=True).copy()"],"id":"fBnaglwvMWIl","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vSrwu3OPB5UH"},"source":["# from category_encoders import TargetEncoder\n","# encoder = TargetEncoder(cols='district')\n","# train_final['district'] = encoder.fit_transform(train_final['district'], target)\n","# data_combined2['street_encoded'] = encoder.fit_transform(data_combined2['street'], target)"],"id":"vSrwu3OPB5UH","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eUKO3DOdKbS0"},"source":["## Feature Selection/ Feature importance\n","* Dropping Constant Features\n","* Dropping Highly Correlated Features\n","* Mutual information gain"],"id":"eUKO3DOdKbS0"},{"cell_type":"code","metadata":{"id":"q72eSZkiKhnE"},"source":["# Constant Features\n","from sklearn.feature_selection import VarianceThreshold\n","var_thres=VarianceThreshold(threshold=0)\n","var_thres.fit(train_final)\n","constant_columns = [column for column in train_final.columns\n","                    if column not in train_final.columns[var_thres.get_support()]]\n","constant_columns"],"id":"q72eSZkiKhnE","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w_Hi5VNoSBTK"},"source":["# Pearson Correlation\n","plt.figure(figsize=(30,20))\n","cor = train_final.corr()\n","sns.heatmap(cor, annot=True, cmap=plt.cm.CMRmap_r)\n","plt.show()"],"id":"w_Hi5VNoSBTK","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qq_hKJqWSrxD"},"source":["# with the following function we can select highly correlated features\n","# it will remove the first feature that is correlated with anything other feature\n","\n","def correlation(dataset, threshold):\n","    col_corr = set()  # Set of all the names of correlated columns\n","    corr_matrix = dataset.corr()\n","    for i in range(len(corr_matrix.columns)):\n","        for j in range(i):\n","            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n","                colname = corr_matrix.columns[i]  # getting the name of column\n","                col_corr.add(colname)\n","    return col_corr"],"id":"Qq_hKJqWSrxD","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ACUtr4NwSuSh"},"source":["corr_features = correlation(train_final, 0.9)\n","len(corr_features)\n","corr_features"],"id":"ACUtr4NwSuSh","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jSouCLfDWisY"},"source":["train_final = train_final.drop(corr_features, axis=1)\n","test_final = test_final.drop(corr_features, axis=1)"],"id":"jSouCLfDWisY","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zyaTU-woTD1i"},"source":["from sklearn.feature_selection import mutual_info_regression\n","\n","# determine the mutual information\n","mutual_info = mutual_info_regression(train_final, log_target)\n","mutual_info"],"id":"zyaTU-woTD1i","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4bFxK7-ITNEC"},"source":["mutual_info = pd.Series(mutual_info)\n","mutual_info.index = train_final.columns\n","mutual_info.sort_values(ascending=False)"],"id":"4bFxK7-ITNEC","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ouh3WJQ9TS2B"},"source":["mutual_info.sort_values(ascending=False).plot.bar(figsize=(15,5))"],"id":"Ouh3WJQ9TS2B","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FiBQ_NdcTyN3"},"source":["from sklearn.feature_selection import SelectPercentile\n","\n","# Select Top N percent features -> Here 80 percent\n","selected_top_columns = SelectPercentile(mutual_info_regression, percentile=80)\n","selected_top_columns.fit(train_final, log_target)"],"id":"FiBQ_NdcTyN3","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"saCjSgNpUDSQ"},"source":["top_n_Columns = train_final.columns[selected_top_columns.get_support()]\n","columns_not_in_top = train_final.columns[~selected_top_columns.get_support()]\n","print(top_n_Columns)\n","print(columns_not_in_top)"],"id":"saCjSgNpUDSQ","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CtJLqIp3bF4b"},"source":["# Categorical features\n","# categorical_columns = pd.Series(column_description[column_description['type'] == 'categorical']['name']).tolist()\n","# categorical_columns"],"id":"CtJLqIp3bF4b","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H-Gf09aPVEJn"},"source":["## Drop Non Imporant columns from both Train and Test data set"],"id":"H-Gf09aPVEJn"},{"cell_type":"code","metadata":{"id":"-L0-og3iVKo_"},"source":["train_final = train_final.drop(columns_not_in_top, axis=1)\n","test_final = test_final.drop(columns_not_in_top, axis=1)"],"id":"-L0-og3iVKo_","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y3Y6d7wBfHN4"},"source":["## Pivot string rows, Hot encoding and dummy variables"],"id":"Y3Y6d7wBfHN4"},{"cell_type":"code","metadata":{"id":"6Jsdbz67b2if"},"source":["data_combined3 = pd.concat([train_final, test_final]).reset_index(drop=True)"],"id":"6Jsdbz67b2if","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Af2Wug9b26r"},"source":["from sklearn.preprocessing import OneHotEncoder\n","\n","def convert_categorical(data_frame: pd.DataFrame):\n","  # Categorical features\n","  data_frame = data_frame.copy()\n","  categorical_columns = list(set(pd.Series(column_description[column_description['type'] == 'categorical']['name']).tolist()) - set([\n","    'layout', \n","    'windows_court', \n","    'windows_street', \n","    'phones',\n","    'elevator_passenger', \n","    'garbage_chute', \n","    'heating',\n","    'district'\n","]))\n","\n","  for column in categorical_columns:\n","    cats = column_description[column_description['name'] == column]['cats']\n","    data_frame[column] = data_frame[column].apply(lambda data: np.stack(cats)[0][int(data)])\n","\n","  return data_frame"],"id":"-Af2Wug9b26r","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"14F9iDbuOBEN"},"source":["def one_hot_encoding(data_frame: pd.DataFrame):\n","  data_frame = data_frame.copy()\n","  categorical_columns = list(set(pd.Series(column_description[column_description['type'] == 'categorical']['name']).tolist()) - set([\n","    'layout', \n","    'windows_court', \n","    'windows_street', \n","    'phones',\n","    'elevator_passenger', \n","    'garbage_chute', \n","    'heating',\n","    'district'\n","]))\n","\n","  for column in categorical_columns:\n","    slice_data_frame = data_frame[[column]]\n","    dummy = pd.get_dummies(slice_data_frame, columns=[column]).iloc[:, :-1]\n","    data_frame.drop([column], axis=1, inplace=True)\n","    data_frame = pd.concat([data_frame, dummy], axis=1)\n","  return data_frame"],"id":"14F9iDbuOBEN","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QnfyuamIhW1y"},"source":["data_combined4 = convert_categorical(data_combined3)"],"id":"QnfyuamIhW1y","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Od5ayFQ9P0Tw"},"source":["data_combined5 = one_hot_encoding(data_combined4)"],"id":"Od5ayFQ9P0Tw","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"snyZ9IiLGNr-"},"source":["## Scaling"],"id":"snyZ9IiLGNr-"},{"cell_type":"code","metadata":{"id":"Qxf7Uz0V9Hl0"},"source":["from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","scaler.fit(data_combined5)\n","\n","data_combined6 = pd.DataFrame(scaler.transform(data_combined5), index=data_combined5.index, columns=data_combined5.columns)"],"id":"Qxf7Uz0V9Hl0","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xAcFOSu-fHN4"},"source":["train_final = data_combined6.loc[:data.index.max(), :]\n","test_final = data_combined6.loc[data.index.max() + 1:, :].reset_index(drop=True).copy()"],"id":"xAcFOSu-fHN4","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0fd3a475"},"source":["## 4: Getting Started\n","\n","To conclude this demo, we will give quick rundown of how you can split the dataset to do your own evaluations, train simple models, and create a submission with predictions that can be uploaded to Kaggle/Blackboard.\n","\n","\n","### Validation Split\n","\n","Even though you can get feedback on how well you're doing by uploading submissions to Kaggle, it a lot quicker to do it yourself. Creating an evaluation split allows you to estimate the out-of-sample performance of your models and is particularly useful when (e.g.) testing out a new preprocessing pipeline or optimizing hyperparameters. In the following cell, we split the training data into two subpartitions with 67% data in the training set and 33% data in the validation set. \n","\n","Note that we stratify based on (rounded) log price. This ensures that we get roughly the same price distributions in both the training and validation set (see figures)."],"id":"0fd3a475"},{"cell_type":"code","metadata":{"id":"0vyFHpmIfHN5"},"source":["from pycaret.regression import setup, compare_models\n","\n","# Model selection\n","# _ = setup(data=pd.concat([train_final, log_target], axis=1), target='price')"],"id":"0vyFHpmIfHN5","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FdeBmrOWfHN5"},"source":["# compare_models()"],"id":"FdeBmrOWfHN5","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8PgHyyTpfHN6"},"source":["# Extra Trees Regressor\n","# Extreme Gradient Boosting\n","# Random Forest Regressor\n","# Light Gradient Boosting Machine\n","# Bayesian Ridge\n","# Ridge Regression\n","# Least Angle Regression\n","# CatBoost Regressor"],"id":"8PgHyyTpfHN6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u6nM6J-5fHN6"},"source":["from catboost import CatBoostRegressor\n","from xgboost import XGBRegressor\n","from lightgbm import LGBMRegressor\n","from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\n","from sklearn.linear_model import BayesianRidge, Ridge, Lars\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import cross_val_score"],"id":"u6nM6J-5fHN6","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6m8F_fnYfHN6"},"source":["## Try with baseline best model -> ExtraTreesRegressor"],"id":"6m8F_fnYfHN6"},{"cell_type":"code","metadata":{"id":"uO70bBWrfHN6"},"source":["# baseline_model = ExtraTreesRegressor()\n","# baseline_model.fit(train_final, log_target)"],"id":"uO70bBWrfHN6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eoIdO-NxfHN6"},"source":["# predictions = 10 ** baseline_model.predict(test_final)\n","# submission = pd.concat([data_test['id'], pd.Series(predictions, name='price_prediction')], axis=1)"],"id":"eoIdO-NxfHN6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n1uo8rXvfHN6"},"source":["# submission.to_csv('./007_submission.csv', index=False, header=True)"],"id":"n1uo8rXvfHN6","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DBExUX4DfHN7"},"source":["## Try with Catboost"],"id":"DBExUX4DfHN7"},{"cell_type":"code","metadata":{"id":"3rF-7HyWfHN7"},"source":["# catboost_model = CatBoostRegressor()\n","# catboost_model.fit(train_final, log_target)"],"id":"3rF-7HyWfHN7","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vUE3HBA7fHN7"},"source":["# catboost_predictions = 10 ** catboost_model.predict(test_final)\n","# catboost_submission = pd.concat([data_test['id'], pd.Series(catboost_predictions, name='price_prediction')], axis=1)"],"id":"vUE3HBA7fHN7","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6P_v7AAefHN7"},"source":["# catboost_submission.to_csv('./007_submission.csv', index=False, header=True)"],"id":"6P_v7AAefHN7","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4wAujnVJfHN7"},"source":["## Hyperparameter optimization"],"id":"4wAujnVJfHN7"},{"cell_type":"code","metadata":{"id":"VzztR5zzfHN7"},"source":["# from sklearn.model_selection import train_test_split\n","# X_train, X_test, y_train, y_test = train_test_split(train_final, log_target, test_size=0.33, random_state=42)"],"id":"VzztR5zzfHN7","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zx7es3zjfHN7"},"source":["# CatBoost Regressor Hyperparameter\n","# def catbr_objective(trial):\n","#     iterations = trial.suggest_int('iterations', 10, 1000)\n","#     learning_rate = trial.suggest_float('learning_rate', 0.01, 0.1)\n","#     depth = trial.suggest_int('depth', 1, 16)\n","#     l2_leaf_reg = trial.suggest_int('l2_leaf_reg', 1, 100)\n","#     # eval_metric\n","#     early_stopping_rounds = trial.suggest_int('early_stopping_rounds', 1, 500)\n","#     random_seed = trial.suggest_int('random_seed', 1, 100)\n","\n","#     catbr_model = CatBoostRegressor(\n","#         iterations=iterations, \n","#         learning_rate=learning_rate,\n","#         depth = depth,\n","#         l2_leaf_reg = l2_leaf_reg,\n","#         early_stopping_rounds = early_stopping_rounds,\n","#         random_seed = random_seed,\n","#         verbose = 0\n","#     )\n","#     catbr_model.fit(X_train, y_train)\n","#     catbr_pred = catbr_model.predict(X_test)\n","#     catbr_score = root_mean_squared_log_error(y_true=y_test, y_pred=catbr_pred)\n","#     return catbr_score\n","\n","# study = optuna.create_study(direction='minimize')\n","# study.optimize(catbr_objective, n_trials=100)\n","\n","# {'iterations': 932,\n","#  'learning_rate': 0.08432987086369166,\n","#  'depth': 9,\n","#  'l2_leaf_reg': 1,\n","#  'early_stopping_rounds': 186,\n","#  'random_seed': 100}"],"id":"Zx7es3zjfHN7","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tPCFrmOwfHN8"},"source":["# Extra Trees Regressor Hyperparameter\n","# def etr_objective(trial):\n","#     n_estimators = trial.suggest_int('n_estimators', 10, 1000)\n","#     max_depth = trial.suggest_int('max_depth', 1, 100)\n","#     min_samples_split = trial.suggest_int('min_samples_split', 2, 100)\n","#     min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 100)\n","\n","#     etr_model = ExtraTreesRegressor(\n","#         n_estimators=n_estimators,\n","#         max_depth=max_depth,\n","#         min_samples_split=min_samples_split,\n","#         min_samples_leaf=min_samples_leaf\n","#     )\n","#     etr_model.fit(X_train, y_train)\n","#     etr_pred = etr_model.predict(X_test)\n","#     etr_score = root_mean_squared_log_error(y_true=y_test, y_pred=etr_pred)\n","#     return etr_score\n","\n","# study_etr = optuna.create_study(direction='minimize')\n","# study_etr.optimize(etr_objective, n_trials=100)"],"id":"tPCFrmOwfHN8","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TSEexqxKfHN8"},"source":["# Random Forest Regressor Hyperparameter\n","# def rfr_objective(trial):\n","#     n_estimators = trial.suggest_int('n_estimators', 10, 1000)\n","#     max_depth = trial.suggest_int('max_depth', 1, 100)\n","#     min_samples_split = trial.suggest_int('min_samples_split', 2, 100)\n","#     min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 100)\n","\n","#     rfr_model = RandomForestRegressor(\n","#         n_estimators=n_estimators,\n","#         max_depth=max_depth,\n","#         min_samples_split=min_samples_split,\n","#         min_samples_leaf=min_samples_leaf\n","#     )\n","#     rfr_model.fit(X_train, y_train)\n","#     rfr_pred = rfr_model.predict(X_test)\n","#     rfr_score = root_mean_squared_log_error(y_true=y_test, y_pred=rfr_pred)\n","#     return rfr_score\n","\n","# study_rfr_name = 'rfr_study'\n","# study_rfr_path = '/content/drive/MyDrive/Colab Notebooks/House-Price-Data/optuna/{}.db'.format(study_rfr_name)\n","# study_rfr = optuna.create_study(\n","#     direction='minimize',\n","#     study_name=study_rfr_name, \n","#     storage='sqlite:///{}'.format(study_rfr_path),\n","#     load_if_exists=True\n","# )\n","# study_rfr.optimize(rfr_objective, n_trials=100)\n","# {'max_depth': 21,\n","#  'min_samples_leaf': 1,\n","#  'min_samples_split': 3,\n","#  'n_estimators': 987}"],"id":"TSEexqxKfHN8","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9JUSmfQAfHN8"},"source":["# Extreme Gradient Boosting Hyperparameter\n","# def egb_objective(trial):\n","#     max_depth = trial.suggest_int('max_depth', 1, 6)\n","#     min_child_weight = trial.suggest_int('min_child_weight', 1, 100)\n","#     eta = trial.suggest_float('eta', 0, 1)\n","#     subsample = trial.suggest_float('subsample', 0, 1)\n","#     colsample_bytree = trial.suggest_float('colsample_bytree', 0, 1)\n","\n","#     egb_model = XGBRegressor(\n","#         max_depth=max_depth,\n","#         min_child_weight=min_child_weight,\n","#         eta=eta,\n","#         subsample=subsample,\n","#         colsample_bytree=colsample_bytree\n","#     )\n","#     egb_model.fit(X_train, y_train)\n","#     egb_pred = egb_model.predict(X_test)\n","#     egb_score = root_mean_squared_log_error(y_true=y_test, y_pred=egb_pred)\n","#     return egb_score\n","\n","# study_egb = optuna.create_study(direction='minimize')\n","# study_egb.optimize(egb_objective, n_trials=100)"],"id":"9JUSmfQAfHN8","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_OIvT-YhfHN9"},"source":["# Light Gradient Boosting Machine Hyperparameter\n","# def lgb_objective(trial):\n","#     max_depth=trial.suggest_int('max_depth', 10, 100)\n","#     num_leaves=trial.suggest_int('num_leaves', 10, 1000)\n","#     min_data_in_leaf=trial.suggest_int('min_data_in_leaf', 10, 500)\n","#     feature_fraction=trial.suggest_float('feature_fraction', 0, 1)\n","#     n_estimators=trial.suggest_int('n_estimators', 10, 1000)\n","\n","#     lgb_model = LGBMRegressor(\n","#         max_depth=max_depth,\n","#         num_leaves=num_leaves,\n","#         min_data_in_leaf=min_data_in_leaf,\n","#         feature_fraction=feature_fraction,\n","#         n_estimators=n_estimators\n","#     )\n","#     lgb_model.fit(X_train, y_train)\n","#     lgb_pred = lgb_model.predict(X_test)\n","#     lgb_score = root_mean_squared_log_error(y_true=y_test, y_pred=lgb_pred)\n","#     return lgb_score\n","\n","# study_lgb_name = 'lgb_study'\n","# study_lgb_path = '/content/drive/MyDrive/Colab Notebooks/House-Price-Data/optuna/{}.db'.format(study_lgb_name)\n","# study_lgb = optuna.create_study(\n","#     direction='minimize',\n","#     study_name=study_lgb_name, \n","#     storage='sqlite:///{}'.format(study_lgb_path),\n","#     load_if_exists=True\n","# )\n","# study_lgb.optimize(lgb_objective, n_trials=100)\n","# {'feature_fraction': 0.7083663819506548,\n","#  'max_depth': 102,\n","#  'min_data_in_leaf': 10,\n","#  'n_estimators': 858,\n","#  'num_leaves': 36}\n","# {'feature_fraction': 0.3086242585990003,\n","#  'max_depth': 65,\n","#  'min_data_in_leaf': 57,\n","#  'n_estimators': 788,\n","#  'num_leaves': 1000}"],"id":"_OIvT-YhfHN9","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j_zm4TFK6hKY"},"source":["hyperparameters = {\n","    \"CatBoost Regressor\": {\n","        'iterations': 932,\n","        'learning_rate': 0.08432987086369166,\n","        'depth': 9,\n","        'l2_leaf_reg': 1,\n","        'early_stopping_rounds': 186,\n","        'random_seed': 100\n","    },\n","    \"Extra Trees Regressor\": {\n","        'max_depth': 80,\n","        'min_samples_leaf': 1,\n","        'min_samples_split': 2,\n","        'n_estimators': 186\n","     },\n","    \"Random Forest Regressor\": {\n","        'max_depth': 21,\n","        'min_samples_leaf': 1,\n","        'min_samples_split': 3,\n","        'n_estimators': 987\n","    }, \n","    \"Extreme Gradient Boosting\": {\n","        'colsample_bytree': 0.6595506783700232,\n","        'eta': 0.27011677687202235,\n","        'max_depth': 6,\n","        'min_child_weight': 8,\n","        'subsample': 0.8754078260405858,\n","        'objective': 'reg:squarederror'\n","    },   \n","    \"Light Gradient Boosting Machine\": {\n","        'feature_fraction': 0.3086242585990003,\n","        'max_depth': 65,\n","        'min_data_in_leaf': 57,\n","        'n_estimators': 788,\n","        'num_leaves': 1000\n","    }\n","}"],"id":"j_zm4TFK6hKY","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NJKPDDuffHN9"},"source":["# Extra Trees Regressor\n","# Extreme Gradient Boosting\n","# Random Forest Regressor\n","# Light Gradient Boosting Machine\n","# Bayesian Ridge\n","# Ridge Regression\n","# Least Angle Regression\n","# CatBoost Regressor\n","models = {\n","    \"CatBoost Regressor\": {'Con': CatBoostRegressor(**hyperparameters['CatBoost Regressor'], verbose=0), 'Weight': 0.4},\n","    \"Extra Trees Regressor\": {'Con': ExtraTreesRegressor(**hyperparameters['Extra Trees Regressor']), 'Weight': 0.2},\n","    \"Random Forest Regressor\": {'Con': RandomForestRegressor(**hyperparameters['Random Forest Regressor']), 'Weight': 0.2}, \n","    \"Extreme Gradient Boosting\": {'Con': XGBRegressor(**hyperparameters['Extreme Gradient Boosting']), 'Weight': 0.1},   \n","    \"Light Gradient Boosting Machine\": {'Con': LGBMRegressor(**hyperparameters['Light Gradient Boosting Machine']), 'Weight': 0.1},\n","    \n","    # \"Bayesian Ridge\": {'Con': BayesianRidge(), 'Weight': 0.1},\n","    # \"Ridge Regression\": {'Con': Ridge(alpha=1.0), 'Weight': 0.1},\n","    # \"Least Angle Regression\": {'Con': Lars(), 'Weight': 0.1}\n","}"],"id":"NJKPDDuffHN9","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0PEMf8dHfHN9"},"source":["## Check which models are doing good"],"id":"0PEMf8dHfHN9"},{"cell_type":"code","metadata":{"id":"lMZYUgkIfHN9"},"source":["# # # # Train test split\n","# from sklearn.model_selection import train_test_split\n","# X_train, X_test, y_train, y_test = train_test_split(train_final, log_target, test_size=0.33, random_state=42)\n","\n","# for name, model in models.items():\n","#     model['Con'].fit(X_train, y_train)\n","#     print(name + \" is trained\")\n","\n","# # # # Predict for each model\n","# results = pd.DataFrame(columns=['Model', 'Result', 'Error'])\n","\n","# for name, model in models.items():\n","#     result = model['Con'].predict(X_test)\n","#     results = results.append({'Model': name, 'Result': result, 'Weight': model['Weight'], 'Error': root_mean_squared_log_error(y_true=y_test, y_pred=result)}, ignore_index=True)\n","\n","# # results.sort_values(by=['Error'], ascending=True).head(5)\n","\n","# results['Result'] = results['Result'] * results['Weight']\n","# root_mean_squared_log_error(y_true=y_test, y_pred=results['Result'].sum())"],"id":"lMZYUgkIfHN9","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MU5dbOiQfHN9"},"source":["## Try to combine all model with bagging"],"id":"MU5dbOiQfHN9"},{"cell_type":"code","metadata":{"id":"-C61ULzxfHN9"},"source":["for name, model in models.items():\n","    model['Con'].fit(train_final, log_target)\n","    print(name + \" is trained\")"],"id":"-C61ULzxfHN9","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X1xyRjG5fHN9"},"source":["results = pd.DataFrame(columns=['Model', 'Result'])\n","\n","for name, model in models.items():\n","    result = 10 ** model['Con'].predict(test_final)\n","    results = results.append({'Model': name, 'Result': model['Weight'] * result }, ignore_index=True)\n","\n","\n","bagging_submission = pd.concat([data_test['id'], pd.Series(results['Result'].sum(), name='price_prediction')], axis=1)"],"id":"X1xyRjG5fHN9","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w2ZYszaEfHN9"},"source":["bagging_submission.to_csv(f'{base_path}/Colab_Restart.1.csv', index=False, header=True)"],"id":"w2ZYszaEfHN9","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3fa52f19"},"source":["# import sklearn.model_selection as model_selection\n","\n","# data_train, data_valid = model_selection.train_test_split(data, test_size=0.33, stratify=np.log(data.price).round())\n","# fig, (ax1, ax2) = plt.subplots(figsize=(12, 4), ncols=2, dpi=100)\n","# print(f'Split dataset into {len(data_train)} training samples and {len(data_valid)} validation samples')\n","\n","# sns.histplot(np.log10(data_train.price).rename('log10(price)'), ax=ax1)\n","# sns.histplot(np.log10(data_valid.price).rename('log10(price)'), ax=ax2)\n","# ax1.set_title('Training set log prices')\n","# ax2.set_title('Validation set log prices')"],"id":"3fa52f19","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"21ea6f22"},"source":["### A very simple Baseline\n","\n","It is useful to establish where the floor is, performance-wise, before starting to hypothesising about what the best features and models should look like. Here we create a simple mean predictor and obtain an RMSLE of around 1.02, both in- and out-of-sample. Any model that is conditioned on features should perform better than this benchmark.\n","\n","**Challenge**: Can you make a different \"constant value predictor\" that performs better than the sample mean with respect the the RMSLE metric used? The solution to this problem could be useful when defining the objective/loss of more complex models."],"id":"21ea6f22"},{"cell_type":"code","metadata":{"id":"b7f47fd9"},"source":["# y_train = data_train.price\n","# y_valid = data_valid.price\n","\n","# mean = y_train.mean()\n","# y_train_hat = np.full(len(y_train), mean)\n","# y_valid_hat = np.full(len(y_valid), mean)\n","\n","# print(f'Train rmsle: {root_mean_squared_log_error(y_true=y_train, y_pred=y_train_hat) :.4f}')\n","# print(f'Valid rmsle: {root_mean_squared_log_error(y_true=y_valid, y_pred=y_valid_hat) :.4f}')"],"id":"b7f47fd9","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d17083a2"},"source":["### Spatial Decision Tree\n","\n","Next we will try to make a slightly more sophisticated model. The EDA above suggests that you can make a nontrivial guess about prices based on geographical location alone. We, therefore, train a model to make price predictions conditioned on the latitude and longitude of the listed apartment. We use a [Regression Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) (Decision Tree for real valued output) and fit it with default hyperparameters.\n","\n","It can be observed that this leads to a substantial improvement in performance. Although we overfit a bit, the validation RMSLE is more than halved compared to the mean-predicting baseline. "],"id":"d17083a2"},{"cell_type":"code","metadata":{"id":"b67a6afb"},"source":["# import sklearn.tree as tree\n","\n","# X_train = data_train[['latitude', 'longitude']]\n","# y_train = data_train.loc[X_train.index].price\n","# X_valid = data_valid[['latitude', 'longitude']]\n","# y_valid = data_valid.loc[X_valid.index].price\n","\n","# model = tree.DecisionTreeRegressor().fit(X_train, y_train)\n","\n","# y_train_hat = model.predict(X_train)\n","# y_valid_hat = model.predict(X_valid)\n","# print(f'Train RMSLE: {root_mean_squared_log_error(y_true=y_train, y_pred=y_train_hat) :.4f}')\n","# print(f'Valid RMSLE: {root_mean_squared_log_error(y_true=y_valid, y_pred=y_valid_hat) :.4f}')\n","\n","# lats, lngs = np.meshgrid(np.linspace(55.5, 56, 50), np.linspace(37, 38, 50))\n","# preds = model.predict(np.stack([lats, lngs], axis=-1).reshape(-1, 2)).reshape(lats.shape)\n","# ax, norm = plot_map(data, a=0.25)\n","# ax.set_title(ax.get_title() + ' with model prediction overlay')\n","# ax.imshow(preds.T[::-1, :], extent=(37, 38, 55.5, 56), alpha=0.3, aspect='auto', cmap='autumn', norm=norm);"],"id":"b67a6afb","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"44f77e22"},"source":["### Sample Submission\n","\n","Lastly, we'll demonstrate how to make a submission. We begin by training another Decision Tree Regressor just like in the section above, except this time over the entire training set. \n","The trained model is then used to generate predictions for the test set. Note that we invoke a little bit of trickery here. The test set is missing latitude/longitude coordinates for two datapoints, so we take the easy route and only generate predictions for the remaining 9935 entries. The two remaining predictions are simply filled with the mean price.\n","In the end, we export the predictions to a .csv file. Its format is pretty simple; one row for each datapoint and each row contains the id of the predicted datapoint along with the predicted price. \n","\n","**Important**: make sure that the id column of your submission file matches the id of `apartments_test.csv` and that the header used is `id,price_prediction`.\n","\n","The exported .csv file can then be submitted to kaggle and bundled with your final delivery on blackboard."],"id":"44f77e22"},{"cell_type":"code","metadata":{"id":"629d878e"},"source":["# Fit model to the full dataset \n","# X_train = data[['latitude', 'longitude']]\n","# y_train = data['price']\n","# y_train\n","# print(f'Num nans in train {X_train.isna().any(axis=1).sum()}')\n","# model = tree.DecisionTreeRegressor(max_depth=20).fit(X_train, y_train)\n","\n","# # Generate predictions for test set \n","# X_test = data_test[['latitude', 'longitude']]\n","# X_test_nan = X_test.isna().any(axis=1)\n","# print(f'Num nans in test: {X_test_nan.sum()}')\n","# y_test_hat = model.predict(X_test[~X_test_nan])\n","\n","# # Construct submission dataframe\n","# submission = pd.DataFrame()\n","# submission['id'] = data_test.id\n","# submission.loc[~X_test_nan, 'price_prediction'] = y_test_hat # Predict on non-nan entries\n","# submission['price_prediction'].fillna(y_train.mean(), inplace=True) # Fill missing entries with mean predictor\n","# print(f'Generated {len(submission)} predictions')\n","\n","# # Export submission to csv with headers\n","# submission.to_csv('sample_submission.csv', index=False)\n","\n","# # Look at submitted csv\n","# print('\\nLine count of submission')\n","# !wc -l sample_submission.csv\n","\n","# print('\\nFirst 5 rows of submission')\n","# !head -n 5 sample_submission.csv"],"id":"629d878e","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"83d2d069"},"source":["### Uploading restults\n","\n","Once a submission csv has been created it can be uploaded to Kaggle. You can upload sumbissions manually through the competition web page ([as explained here](https://www.kaggle.com/docs/competitions#submitting-predictions)). Alternatively, you can use the Kaggle API and do it from the terminal with the following command template:\n","\n","```bash\n","kaggle competitions submit moscow-housing-tdt4173 -f <filepath> -m \"<message>\"\n","```\n","\n","Where `<filepath>` in this case would be `./sample_submission.csv` and `<message>` is your own comment for the submission.\n","\n","The submission created here was actually used as the sample submission in the Kaggle competition. You should be able to find it in the leaderboard under the team name `sample_submission.csv`. It obtained a **public**$^1$ score of 0.61666. Note that this is quite a bit higher than the 0.4326 validation set score we obtained above, but it is actually not that surprising given the way this model work (hint: analyze building IDs across train/test).\n","\n","$^1$) The kaggle leaderboard will display your public score. For this competition, this constitutes RMLSE calcluated over a sub-sample of the test set (50% of the data). Your score for the remaining test set datapoints will remain hidden until the end of the competition. The reason for this is the same reason that you typically want to do a train/valid/test split in machine learning. If complete feedback on all the test samples were constantly available, then you could more easily overfit you models (in terms of hyperparamters, ensemble composition, etc)."],"id":"83d2d069"}]}